# ğŸ•µï¸â€â™‚ï¸ **RESPONSIBLE AI DETECTIVE BLOG**

Welcome to todayâ€™s case files, where I put on my digital trench coat and investigate some AI systems that might look â€œsmartâ€ but could be hiding shady practices.

---

## Case File #1: **The Biased Bot** ğŸ¤–ğŸ’¼

### **Whatâ€™s happening**
A company uses an AI bot to go through job applications. Itâ€™s trained on past hiring data, so it â€œlearnsâ€ who usually gets hired.

### **Whatâ€™s problematic**
Turns out, the bot often rejects women who took career breaks (for caregiving, study, or health reasons). The AI assumes a gap = â€œbad candidate.â€ Basically, itâ€™s recycling societyâ€™s past biases and scaling them up with machine efficiency. ğŸš©

### **One improvement idea**
Redesign the system to ignore irrelevant features like career gaps. Instead, focus on skills and achievements. Bonus move: run regular fairness audits to check if rejection rates differ across gender, race, or age groups.

---

## Case File #2: **The Shifty Proctor** ğŸ“ğŸ‘€

### **Whatâ€™s happening**
During online exams, an AI monitors webcams and flags â€œsuspiciousâ€ behavior â€” like looking away from the screen too often.

### **Whatâ€™s problematic**
Neurodivergent students or those with anxiety might have unusual eye movement or fidgeting habits. The AI flags them unfairly as cheaters. In other words, students get punished for being themselves. ğŸš¨

### **One improvement idea**
Instead of relying only on eye tracking, combine multiple signals (like keyboard activity or exam logs). And most importantly, keep a **human in the loop** â€” so a teacher makes the final call, not just an algorithm.


