# 🕵️‍♂️ **RESPONSIBLE AI DETECTIVE BLOG**

Welcome to today’s case files, where I put on my digital trench coat and investigate some AI systems that might look “smart” but could be hiding shady practices.

---

## Case File #1: **The Biased Bot** 🤖💼

### **What’s happening**
A company uses an AI bot to go through job applications. It’s trained on past hiring data, so it “learns” who usually gets hired.

### **What’s problematic**
Turns out, the bot often rejects women who took career breaks (for caregiving, study, or health reasons). The AI assumes a gap = “bad candidate.” Basically, it’s recycling society’s past biases and scaling them up with machine efficiency. 🚩

### **One improvement idea**
Redesign the system to ignore irrelevant features like career gaps. Instead, focus on skills and achievements. Bonus move: run regular fairness audits to check if rejection rates differ across gender, race, or age groups.

---

## Case File #2: **The Shifty Proctor** 🎓👀

### **What’s happening**
During online exams, an AI monitors webcams and flags “suspicious” behavior — like looking away from the screen too often.

### **What’s problematic**
Neurodivergent students or those with anxiety might have unusual eye movement or fidgeting habits. The AI flags them unfairly as cheaters. In other words, students get punished for being themselves. 🚨

### **One improvement idea**
Instead of relying only on eye tracking, combine multiple signals (like keyboard activity or exam logs). And most importantly, keep a **human in the loop** — so a teacher makes the final call, not just an algorithm.


